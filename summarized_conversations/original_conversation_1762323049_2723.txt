{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="HEX-Control-Nexus/backend/python_core/scraper.py">\n"""\nHEX Control Nexus - Web Scraper Module\nHandles static and dynamic web scraping tasks\n"""\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport random\nimport logging\nimport json\nimport csv\nimport sqlite3\nimport os\nfrom urllib.parse import urljoin, urlparse\nfrom typing import Dict, List, Optional\nimport re\n\nlogger = logging.getLogger(__name__)\n\nclass Scraper:\n    def __init__(self, target_config: Dict, global_config: Dict):\n        self.target = target_config\n        self.config = global_config\n        self.session = requests.Session()\n        self.setup_session()\n        \n    def setup_session(self):\n        """Setup session with user agents and headers"""\n        user_agents = self.config.get(\'user_agents\', [\n            \'Mozilla/5.0 (compatible; HEX/1.0)\'\n        ])\n        self.session.headers.update({\n            \'User-Agent\': random.choice(user_agents)\n        })\n        \n    def check_robots_txt(self, url: str) -> bool:\n        """Check robots.txt for allowed paths"""\n        try:\n            parsed_url = urlparse(url)\n            robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"\n            \n            response = self.session.get(robots_url, timeout=10)\n            if response.status_code == 200:\n                # Simple check - in reality, you\'d want a proper robots.txt parser\n                if \'Disallow: \' + parsed_url.path in response.text:\n                    logger.warning(f"Path {parsed_url.path} disallowed by robots.txt")\n                    return False\n            return True\n        except Exception as e:\n            logger.warning(f"Could not check robots.txt: {e}")\n            return True  # Default to allowed if we can\'t check\n            \n    def scrape_page(self, url: str) -> Optional[BeautifulSoup]:\n        """Scrape a single page and return BeautifulSoup object"""\n        if not self.check_robots_txt(url):\n            logger.error(f"Scraping disallowed by robots.txt: {url}")\n            return None\n            \n        try:\n            # Apply rate limiting\n            delay = self.config.get(\'rate_limit\', {}).get(\'delay_seconds\', 1.0)\n            if self.config.get(\'rate_limit\', {}).get(\'jitter\', False):\n                delay += random.uniform(0, 1)\n            time.sleep(delay)\n            \n            response = self.session.get(url, timeout=30)\n            response.raise_for_status()\n            \n            return BeautifulSoup(response.content, \'html.parser\')\n        except Exception as e:\n            logger.error(f"Failed to scrape {url}: {e}")\n            return None\n            \n    def extract_data(self, soup: BeautifulSoup, selectors: Dict) -> List[Dict]:\n        """Extract data from soup using CSS selectors"""\n        items = []\n        item_selector = selectors.get(\'item\', \'\')\n        field_selectors = selectors.get(\'fields\', {})\n        \n        item_elements = soup.select(item_selector)\n        \n        for element in item_elements:\n            item_data = {}\n            for field_name, selector in field_selectors.items():\n                try:\n                    if selector.endswith(\'::text\'):\n                        # Extract text\n                        selector = selector[:-7]  # Remove ::text\n                        field_element = element.select_one(selector)\n                        item_data[field_name] = field_element.get_text(strip=True) if field_element else \'\'\n                    elif \'::attr(\' in selector:\n                        # Extract attribute\n                        attr_match = re.search(r\'::attr\\((\\w+)\\)\', selector)\n                        if attr_match:\n                            attr_name = attr_match.group(1)\n                            selector = selector[:attr_match.start()]\n                            field_element = element.select_one(selector)\n                            item_data[field_name] = field_element.get(attr_name, \'\') if field_element else \'\'\n                    else:\n                        # Default to text extraction\n                        field_element = element.select_one(selector)\n                        item_data[field_name] = field_element.get_text(strip=True) if field_element else \'\'\n                except Exception as e:\n                    logger.warning(f"Failed to extract field {field_name}: {e}")\n                    item_data[field_name] = \'\'\n                    \n            items.append(item_data)\n            \n        return items\n        \n    def save_to_csv(self, data: List[Dict], filename: str):\n        """Save data to CSV file"""\n        if not data:\n            return\n            \n        keys = data[0].keys()\n        with open(filename, \'w\', newline=\'\', encoding=\'utf-8\') as f:\n            writer = csv.DictWriter(f, fieldnames=keys)\n            writer.writeheader()\n            writer.writerows(data)\n            \n    def save_to_jsonl(self, data: List[Dict], filename: str):\n        """Save data to JSONL file"""\n        with open(filename, \'w\', encoding=\'utf-8\') as f:\n            for item in data:\n                f.write(json.dumps(item) + \'\\n\')\n                \n    def save_to_sqlite(self, data: List[Dict], db_path: str, table_name: str, unique_key: str = None):\n        """Save data to SQLite database"""\n        if not data:\n            return\n            \n        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n        \n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n        \n        # Create table if it doesn\'t exist\n        if data:\n            columns = list(data[0].keys())\n            column_defs = \', \'.join([f"{col} TEXT" for col in columns])\n            cursor.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs})")\n            \n            # Insert data\n            for item in data:\n                # Check for duplicates if unique_key is specified\n                if unique_key and unique_key in item:\n                    cursor.execute(f"SELECT 1 FROM {table_name} WHERE {unique_key} = ?", \n                                 (item[unique_key],))\n                    if cursor.fetchone():\n                        continue  # Skip duplicates\n                        \n                placeholders = \', \'.join([\'?\' for _ in columns])\n                values = [item.get(col, \'\') for col in columns]\n                cursor.execute(f"INSERT INTO {table_name} VALUES ({placeholders})", values)\n                \n        conn.commit()\n        conn.close()\n        \n    def scrape(self) -> List[Dict]:\n        """Main scraping function"""\n        base_url = self.target[\'base_url\']\n        start_paths = self.target[\'start_paths\']\n        selectors = self.target[\'selectors\']\n        storage_config = self.target.get(\'storage\', {})\n        \n        all_data = []\n        \n        for path in start_paths:\n            url = urljoin(base_url, path)\n            logger.info(f"Scraping: {url}")\n            \n            soup = self.scrape_page(url)\n            if not soup:\n                continue\n                \n            data = self.extract_data(soup, selectors)\n            all_data.extend(data)\n            \n            # Handle pagination\n            pagination = self.target.get(\'pagination\', {})\n            next_selector = pagination.get(\'next_selector\')\n            next_url_template = pagination.get(\'next_url_template\')\n            \n            if next_selector:\n                next_link = soup.select_one(next_selector)\n                if next_link and next_link.get(\'href\'):\n                    next_url = urljoin(base_url, next_link[\'href\'])\n                    # For demo, we\'ll just log the next URL\n                    logger.info(f"Next page: {next_url}")\n                    \n        # Save data\n        storage_type = storage_config.get(\'type\', \'jsonl\')\n        storage_path = storage_config.get(\'path\', f"output_{self.target[\'name\']}.jsonl")\n        unique_key = storage_config.get(\'unique_key\')\n        \n        if storage_type == \'csv\':\n            self.save_to_csv(all_data, storage_path)\n        elif storage_type == \'jsonl\':\n            self.save_to_jsonl(all_data, storage_path)\n        elif storage_type == \'sqlite\':\n            table_name = self.target.get(\'name\', \'scraped_data\')\n            self.save_to_sqlite(all_data, storage_path, table_name, unique_key)\n            \n        logger.info(f"Scraped {len(all_data)} items from {self.target[\'name\']}")\n        return all_data\n</create-file>\n</function_calls>'}